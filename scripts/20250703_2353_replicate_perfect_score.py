#!/usr/bin/env python3
"""
PURPOSE: Replicate the exact 0.975708 score with specific XGBoost parameters from a known solution

HYPOTHESIS: The exact score can be replicated by using the same preprocessing steps and 
XGBoost hyperparameters, particularly the very low learning rate (0.006358) and high 
regularization (reg_alpha=5.5149)

EXPECTED: Achieve CV score very close to 0.975708 and possibly improve with 3000 estimators

RESULT: Successfully replicated the approach with exact parameters, confirming that specific
preprocessing (mean imputation for numerics, NaN for missing categoricals) combined with
highly regularized XGBoost produces consistent ~97.57% accuracy
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, cross_val_score
import warnings
warnings.filterwarnings('ignore')

print("REPLICATING EXACT KAGGLE SCORE: 0.975708")
print("="*60)

# Load data
train_df = pd.read_csv("../../train.csv")
test_df = pd.read_csv("../../test.csv")

# Features (same as in the solution)
features = ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 
            'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 
            'Post_frequency']

# Separate numeric and categorical
numerical_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 
                  'Friends_circle_size', 'Post_frequency']
categorical_cols = ['Stage_fear', 'Drained_after_socializing']

print("\nPreprocessing (exactly as in solution):")
print("-"*40)

# 1. Handle missing values EXACTLY as in solution
print("1. Filling missing values:")

# Numerical: use mean (not 0!)
for col in numerical_cols:
    mean_val = train_df[col].mean()
    train_df[col].fillna(mean_val, inplace=True)
    test_df[col].fillna(mean_val, inplace=True)  # Use train mean for test!
    print(f"   {col}: filled with mean = {mean_val:.4f}")

# Categorical: fill with 'Missing' first
for col in categorical_cols:
    train_df[col].fillna('Missing', inplace=True)
    test_df[col].fillna('Missing', inplace=True)
    print(f"   {col}: filled with 'Missing'")

# 2. Encode categorical variables
print("\n2. Encoding categorical variables:")
mapping_yes_no = {'Yes': 1, 'No': 0}
# Note: 'Missing' will become NaN after mapping!

for col in categorical_cols:
    train_df[col] = train_df[col].map(mapping_yes_no)
    test_df[col] = test_df[col].map(mapping_yes_no)
    print(f"   {col}: Yes->1, No->0, Missing->NaN")

# 3. Encode target
mapping_personality = {'Extrovert': 1, 'Introvert': 0}
train_df['Personality'] = train_df['Personality'].map(mapping_personality)

# Prepare features and target
X = train_df[features]
y = train_df['Personality']
X_test = test_df[features]

print(f"\nData shapes:")
print(f"X_train: {X.shape}")
print(f"X_test: {X_test.shape}")
print(f"Missing values in X_train: {X.isnull().sum().sum()}")
print(f"Missing values in X_test: {X_test.isnull().sum().sum()}")

# EXACT XGBoost parameters from the solution
print("\n" + "="*60)
print("XGBOOST WITH EXACT PARAMETERS FROM SOLUTION")
print("="*60)

xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    tree_method='gpu_hist',           # GPU version
    predictor='gpu_predictor',        # GPU predictor
    gpu_id=0,                         # Use first GPU
    enable_categorical=False,
    random_state=42,
    n_estimators=1000,                # Their exact value
    learning_rate=0.006358,           # Their exact value - very low!
    max_depth=8,                      # Their exact value
    subsample=0.8854,                 # Their exact value
    colsample_bytree=0.6,             # Their exact value
    reg_lambda=0.8295,                # Their exact value
    reg_alpha=5.5149,                 # Their exact value
    gamma=0.0395,                     # Their exact value
    min_child_weight=2,               # Their exact value
    use_label_encoder=False,
    verbosity=1
)

# Cross-validation
print("\nRunning 5-fold cross-validation...")
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(xgb_model, X, y, cv=cv, scoring='accuracy', verbose=1)

print(f"\nCV Scores: {cv_scores}")
print(f"Mean CV Score: {cv_scores.mean():.8f}")
print(f"Std CV Score: {cv_scores.std():.8f}")

# Train on full data
print("\nTraining on full dataset...")
xgb_model.fit(X, y)

# Make predictions
print("\nMaking predictions...")
predictions = xgb_model.predict(X_test)

# Convert back to labels
mapping_inverse = {1: 'Extrovert', 0: 'Introvert'}
pred_labels = [mapping_inverse[pred] for pred in predictions]

# Create submission
submission = pd.DataFrame({
    'id': test_df['id'],
    'Personality': pred_labels
})

# Save submission
filename = f'submission_EXACT_PARAMS_cv{cv_scores.mean():.6f}.csv'
submission.to_csv(filename, index=False)
print(f"\nSaved: {filename}")

# Feature importance
print("\n" + "="*60)
print("FEATURE IMPORTANCE:")
print("="*60)
importances = xgb_model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

# Try with YOUR parameters (3000 estimators)
print("\n" + "="*60)
print("TRYING WITH 3000 ESTIMATORS (YOUR SUGGESTION)")
print("="*60)

xgb_model_3000 = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    tree_method='gpu_hist',
    predictor='gpu_predictor',
    gpu_id=0,
    enable_categorical=False,
    random_state=42,
    n_estimators=3000,                # YOUR value - 3x more!
    learning_rate=0.006358,           # Same low learning rate
    max_depth=8,
    subsample=0.8854,
    colsample_bytree=0.6,
    reg_lambda=0.8295,
    reg_alpha=5.5149,
    gamma=0.0395,
    min_child_weight=2,
    use_label_encoder=False,
    verbosity=0
)

# Quick CV test
print("Testing with 3000 estimators...")
cv_scores_3000 = cross_val_score(xgb_model_3000, X, y, cv=cv, scoring='accuracy', n_jobs=1)
print(f"CV Score with 3000 estimators: {cv_scores_3000.mean():.8f}")

if cv_scores_3000.mean() > cv_scores.mean():
    print("âœ“ 3000 estimators gives better CV score! Training full model...")
    xgb_model_3000.fit(X, y)
    predictions_3000 = xgb_model_3000.predict(X_test)
    pred_labels_3000 = [mapping_inverse[pred] for pred in predictions_3000]
    
    submission_3000 = pd.DataFrame({
        'id': test_df['id'],
        'Personality': pred_labels_3000
    })
    submission_3000.to_csv(f'submission_3000_estimators_cv{cv_scores_3000.mean():.6f}.csv', index=False)
    print(f"Saved: submission_3000_estimators_cv{cv_scores_3000.mean():.6f}.csv")

print("\n" + "="*60)
print("SUMMARY:")
print("="*60)
print("Key differences that might give 0.975708:")
print("1. Numerical missing values filled with mean (not 0)")
print("2. Categorical missing values become NaN (not 0)")
print("3. Very low learning_rate = 0.006358")
print("4. High regularization (reg_alpha = 5.5149)")
print("5. Specific subsample = 0.8854")
print("\nSubmit these files to Kaggle!")