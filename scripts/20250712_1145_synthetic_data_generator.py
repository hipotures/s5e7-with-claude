#!/usr/bin/env python3
"""
Generate synthetic personality data using VAE or GAN
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Paths
DATA_DIR = Path("/mnt/ml/kaggle/playground-series-s5e7/")
SYNTHETIC_DIR = Path("/mnt/ml/kaggle/playground-series-s5e7-synthetic/")
OUTPUT_DIR = Path("/mnt/ml/competitions/2025/playground-series-s5e7/WORKSPACE/scripts/output")

class PersonalityVAE(nn.Module):
    """Variational Autoencoder for personality data"""
    
    def __init__(self, input_dim=7, latent_dim=16):
        super(PersonalityVAE, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32)
        )
        
        self.fc_mu = nn.Linear(32, latent_dim)
        self.fc_logvar = nn.Linear(32, latent_dim)
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim + 1, 32),  # +1 for personality label
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Linear(64, input_dim)
        )
        
    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_logvar(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z, personality):
        # Concatenate latent vector with personality label
        z_with_label = torch.cat([z, personality.unsqueeze(1)], dim=1)
        return self.decoder(z_with_label)
    
    def forward(self, x, personality):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z, personality), mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    """VAE loss = reconstruction loss + KL divergence"""
    recon_loss = nn.MSELoss()(recon_x, x)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + 0.001 * kl_loss

def load_all_data():
    """Load competition data (which is already synthetic!)"""
    print("="*60)
    print("LOADING COMPETITION DATA")
    print("="*60)
    
    # Competition data (already synthetic)
    all_train = pd.read_csv(DATA_DIR / "train.csv")
    print(f"\nCompetition train (synthetic): {all_train.shape}")
    print("Note: This data is already synthetic, generated by Kaggle!")
    
    # Prepare features
    feature_cols = ['Time_spent_Alone', 'Social_event_attendance', 
                   'Friends_circle_size', 'Going_outside', 'Post_frequency',
                   'Stage_fear', 'Drained_after_socializing']
    
    # Encode categorical
    all_train['Stage_fear_encoded'] = (all_train['Stage_fear'] == 'Yes').astype(float)
    all_train['Drained_encoded'] = (all_train['Drained_after_socializing'] == 'Yes').astype(float)
    
    # Numeric features
    numeric_features = ['Time_spent_Alone', 'Social_event_attendance', 
                       'Friends_circle_size', 'Going_outside', 'Post_frequency',
                       'Stage_fear_encoded', 'Drained_encoded']
    
    X = all_train[numeric_features].values
    y = (all_train['Personality'] == 'Introvert').astype(float).values
    
    # Handle missing values
    X = np.nan_to_num(X, nan=np.nanmean(X, axis=0))
    
    return X, y, numeric_features

def train_vae(X, y, epochs=100):
    """Train VAE on combined data"""
    print("\n" + "="*60)
    print("TRAINING VAE")
    print("="*60)
    
    # Scale data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Convert to tensors
    X_tensor = torch.FloatTensor(X_scaled)
    y_tensor = torch.FloatTensor(y)
    
    # Create dataset
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)
    
    # Initialize model
    model = PersonalityVAE(input_dim=X.shape[1], latent_dim=16)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Training loop
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in dataloader:
            optimizer.zero_grad()
            
            recon_x, mu, logvar = model(batch_x, batch_y)
            loss = vae_loss(recon_x, batch_x, mu, logvar)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}")
    
    return model, scaler

def generate_synthetic_samples(model, scaler, n_samples=10000, personality_mix=0.5):
    """Generate new synthetic samples"""
    print("\n" + "="*60)
    print(f"GENERATING {n_samples} SYNTHETIC SAMPLES")
    print("="*60)
    
    model.eval()
    
    # Sample from latent space
    z = torch.randn(n_samples, 16)  # latent_dim = 16
    
    # Create personality labels (balanced or custom mix)
    n_introverts = int(n_samples * personality_mix)
    personalities = torch.zeros(n_samples)
    personalities[:n_introverts] = 1.0
    
    # Shuffle
    perm = torch.randperm(n_samples)
    personalities = personalities[perm]
    
    # Generate samples
    with torch.no_grad():
        generated = model.decode(z, personalities)
        generated_numpy = generated.numpy()
    
    # Inverse transform
    generated_original = scaler.inverse_transform(generated_numpy)
    
    # Create dataframe
    feature_names = ['Time_spent_Alone', 'Social_event_attendance', 
                    'Friends_circle_size', 'Going_outside', 'Post_frequency',
                    'Stage_fear_encoded', 'Drained_encoded']
    
    synthetic_df = pd.DataFrame(generated_original, columns=feature_names)
    
    # Add personality labels
    synthetic_df['Personality'] = personalities.numpy()
    synthetic_df['Personality'] = synthetic_df['Personality'].map({0: 'Extrovert', 1: 'Introvert'})
    
    # Convert encoded features back
    synthetic_df['Stage_fear'] = synthetic_df['Stage_fear_encoded'].apply(lambda x: 'Yes' if x > 0.5 else 'No')
    synthetic_df['Drained_after_socializing'] = synthetic_df['Drained_encoded'].apply(lambda x: 'Yes' if x > 0.5 else 'No')
    
    # Round numeric features to integers (0-15 range)
    numeric_cols = ['Time_spent_Alone', 'Social_event_attendance', 
                   'Friends_circle_size', 'Going_outside', 'Post_frequency']
    
    for col in numeric_cols:
        synthetic_df[col] = np.clip(synthetic_df[col].round(), 0, 15).astype(int)
    
    # Drop encoded columns
    synthetic_df = synthetic_df.drop(['Stage_fear_encoded', 'Drained_encoded'], axis=1)
    
    # Add synthetic ID
    synthetic_df['id'] = range(100000, 100000 + len(synthetic_df))
    
    return synthetic_df

def analyze_synthetic_quality(original_df, synthetic_df):
    """Compare statistics of original vs synthetic data"""
    print("\n" + "="*60)
    print("SYNTHETIC DATA QUALITY ANALYSIS")
    print("="*60)
    
    numeric_cols = ['Time_spent_Alone', 'Social_event_attendance', 
                   'Friends_circle_size', 'Going_outside', 'Post_frequency']
    
    print("\nMean comparison:")
    for col in numeric_cols:
        orig_mean = original_df[col].mean()
        synth_mean = synthetic_df[col].mean()
        diff = abs(orig_mean - synth_mean)
        print(f"{col}: Original={orig_mean:.2f}, Synthetic={synth_mean:.2f}, Diff={diff:.2f}")
    
    print("\nPersonality distribution:")
    orig_intro = (original_df['Personality'] == 'Introvert').mean()
    synth_intro = (synthetic_df['Personality'] == 'Introvert').mean()
    print(f"Original: {orig_intro:.1%} Introverts")
    print(f"Synthetic: {synth_intro:.1%} Introverts")
    
    print("\nCategorical features:")
    for col in ['Stage_fear', 'Drained_after_socializing']:
        orig_yes = (original_df[col] == 'Yes').mean()
        synth_yes = (synthetic_df[col] == 'Yes').mean()
        print(f"{col} (Yes): Original={orig_yes:.1%}, Synthetic={synth_yes:.1%}")

def main():
    # Load all data
    X, y, feature_names = load_all_data()
    
    # Train VAE
    model, scaler = train_vae(X, y, epochs=50)
    
    # Generate synthetic samples
    synthetic_df = generate_synthetic_samples(model, scaler, n_samples=20000)
    
    # Save synthetic data
    output_file = OUTPUT_DIR / "synthetic_vae_generated.csv"
    synthetic_df.to_csv(output_file, index=False)
    print(f"\nSaved synthetic data to: {output_file}")
    
    # Analyze quality
    original_df = pd.read_csv(DATA_DIR / "train.csv")
    analyze_synthetic_quality(original_df, synthetic_df)
    
    print("\n" + "="*60)
    print("NEXT STEPS:")
    print("="*60)
    print("1. Train models on: original + synthetic + VAE-generated data")
    print("2. Use data augmentation during training")
    print("3. Create ensemble with different synthetic mixtures")
    print("4. Test if more diverse training data improves generalization")

if __name__ == "__main__":
    main()