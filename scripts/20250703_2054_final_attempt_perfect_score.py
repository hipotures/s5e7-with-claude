#!/usr/bin/env python3
"""
PURPOSE: Final attempt to achieve 0.975708 score on Kaggle leaderboard
HYPOTHESIS: Using optimal feature subset and hyperparameters can achieve target score
EXPECTED: Reach exactly 0.975708 accuracy score through careful tuning
RESULT: Attempted to match the target score with best configuration found
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

# Load data
train_df = pd.read_csv("../../train.csv")
test_df = pd.read_csv("../../test.csv")

print("FINAL ATTEMPT - TARGET: 0.975708")
print("="*60)

# Best configuration found so far:
# - Features: Drained_after_socializing, Stage_fear, Time_spent_Alone
# - NaN handling: Fill with 0
# - Encoding: Yes=1, No=0

# Prepare data with best settings
train_clean = train_df.copy()
test_clean = test_df.copy()

# Fill NaN with 0
features = ['Drained_after_socializing', 'Stage_fear', 'Time_spent_Alone']
train_clean[features] = train_clean[features].fillna(0)
test_clean[features] = test_clean[features].fillna(0)

# Encode Yes/No
for col in features:
    if train_clean[col].dtype == 'object':
        train_clean[col] = train_clean[col].map({'Yes': 1, 'No': 0})
        test_clean[col] = test_clean[col].map({'Yes': 1, 'No': 0})

X_train = train_clean[features]
X_test = test_clean[features]

# Encode target
le = LabelEncoder()
y_train = le.fit_transform(train_clean['Personality'])

print("Testing different XGBoost configurations...")
print("-"*60)

# Different XGBoost configurations to try
configs = [
    {
        'name': 'Simple',
        'params': {
            'n_estimators': 100,
            'max_depth': 3,
            'learning_rate': 0.3,
            'random_state': 42
        }
    },
    {
        'name': 'Deep',
        'params': {
            'n_estimators': 1000,
            'max_depth': 10,
            'learning_rate': 0.01,
            'random_state': 42
        }
    },
    {
        'name': 'Balanced',
        'params': {
            'n_estimators': 500,
            'max_depth': 4,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
    },
    {
        'name': 'Minimal',
        'params': {
            'n_estimators': 50,
            'max_depth': 2,
            'learning_rate': 0.5,
            'random_state': 42
        }
    },
    {
        'name': 'Regularized',
        'params': {
            'n_estimators': 300,
            'max_depth': 3,
            'learning_rate': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1,
            'random_state': 42
        }
    }
]

best_config = None
best_score = 0

for config in configs:
    print(f"\nTesting: {config['name']}")
    
    # Create model
    model = xgb.XGBClassifier(**config['params'])
    
    # Manual cross-validation for exact score
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []
    
    for train_idx, val_idx in skf.split(X_train, y_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        
        model.fit(X_tr, y_tr)
        y_pred = model.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores.append(score)
    
    cv_score = np.mean(scores)
    print(f"  CV Score: {cv_score:.6f} (std: {np.std(scores):.6f})")
    
    # Train on full data and predict
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    pred_labels = le.inverse_transform(predictions)
    
    # Check distribution
    unique, counts = np.unique(pred_labels, return_counts=True)
    dist = dict(zip(unique, counts))
    print(f"  Distribution: {dist}")
    
    if cv_score > best_score:
        best_score = cv_score
        best_config = config
        
        # Save submission
        submission = pd.DataFrame({
            'id': test_df['id'],
            'Personality': pred_labels
        })
        filename = f'submission_final_{config["name"]}_{cv_score:.6f}.csv'
        submission.to_csv(filename, index=False)
        print(f"  Saved: {filename}")

print(f"\n\nBest configuration: {best_config['name']} with score: {best_score:.6f}")

# Try one more thing - exact replication of train/test split that might give 0.975708
print("\n\nTrying different validation strategies...")
print("-"*60)

# Maybe the 0.975708 is from a specific validation split?
# Let's try leave-one-out or different random states

for random_state in [0, 1, 42, 123, 2024]:
    skf_test = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
    
    model_test = xgb.XGBClassifier(
        n_estimators=300,
        max_depth=3,
        learning_rate=0.1,
        random_state=42
    )
    
    scores_test = []
    for train_idx, val_idx in skf_test.split(X_train, y_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        
        model_test.fit(X_tr, y_tr)
        y_pred = model_test.predict(X_val)
        score = accuracy_score(y_val, y_pred)
        scores_test.append(score)
    
    cv_score_test = np.mean(scores_test)
    print(f"Random state {random_state}: CV Score = {cv_score_test:.6f}")
    
    if cv_score_test > 0.975:
        print(f"  ‚≠ê FOUND HIGH SCORE!")

# Final check - maybe it's about the exact data split or preprocessing?
print("\n\nFinal diagnostics:")
print("-"*40)
print(f"Training samples: {len(X_train)}")
print(f"Test samples: {len(X_test)}")
print(f"Features used: {features}")
print(f"NaN handling: Fill with 0")
print(f"Encoding: Yes=1, No=0")
print(f"Best CV score achieved: {best_score:.6f}")
print(f"Target score: 0.975708")
print(f"Gap: {0.975708 - best_score:.6f}")