#!/usr/bin/env python3
"""
Simplified YDF advanced features focusing on what works
"""

import pandas as pd
import numpy as np
from pathlib import Path
import ydf
import time
from sklearn.model_selection import StratifiedKFold
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# Paths
DATA_DIR = Path("/mnt/ml/kaggle/playground-series-s5e7/")
WORKSPACE_DIR = Path("/mnt/ml/competitions/2025/playground-series-s5e7/WORKSPACE")
OUTPUT_DIR = WORKSPACE_DIR / "scripts/output"
OUTPUT_DIR.mkdir(exist_ok=True)

def prepare_data():
    """Load and prepare data"""
    print("="*60)
    print("LOADING DATA")
    print("="*60)
    
    # Load data
    train_df = pd.read_csv(DATA_DIR / "train.csv")
    test_df = pd.read_csv(DATA_DIR / "test.csv")
    
    # Handle missing values
    for col in ['Stage_fear', 'Drained_after_socializing']:
        train_df[col] = train_df[col].fillna('Unknown')
        test_df[col] = test_df[col].fillna('Unknown')
    
    # Features
    feature_cols = ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 
                    'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 
                    'Post_frequency']
    
    train_data = train_df[feature_cols + ['Personality']].copy()
    test_data = test_df[feature_cols].copy()
    
    print(f"Train shape: {train_data.shape}")
    print(f"Test shape: {test_data.shape}")
    
    return train_data, test_data, train_df, test_df

def hyperparameter_tuning(train_data):
    """Manual hyperparameter tuning"""
    print("\n" + "="*60)
    print("HYPERPARAMETER TUNING")
    print("="*60)
    
    configs = [
        {"name": "Baseline", "num_trees": 300, "max_depth": 16},
        {"name": "Deep", "num_trees": 300, "max_depth": 30},
        {"name": "More Trees", "num_trees": 500, "max_depth": 20},
        {"name": "Very Deep", "num_trees": 400, "max_depth": -1},
        {"name": "Regularized", "num_trees": 300, "max_depth": 10, "min_examples": 20},
        {"name": "Large Ensemble", "num_trees": 1000, "max_depth": 12},
    ]
    
    results = []
    best_model = None
    best_accuracy = 0
    
    for config in configs:
        print(f"\nTesting {config['name']}...")
        
        # Create learner
        learner_params = {
            "label": "Personality",
            "num_trees": config["num_trees"],
            "max_depth": config["max_depth"],
            "winner_take_all": True,
            "random_seed": 42
        }
        
        # Add optional parameters
        if "min_examples" in config:
            learner_params["min_examples"] = config["min_examples"]
        
        learner = ydf.RandomForestLearner(**learner_params)
        
        # Train
        start = time.time()
        model = learner.train(train_data)
        train_time = time.time() - start
        
        # Evaluate
        oob_acc = model.self_evaluation().accuracy
        train_acc = model.evaluate(train_data).accuracy
        
        print(f"  OOB Accuracy: {oob_acc:.6f}")
        print(f"  Train Accuracy: {train_acc:.6f}")
        print(f"  Training time: {train_time:.2f}s")
        
        results.append({
            **config,
            "oob_accuracy": oob_acc,
            "train_accuracy": train_acc,
            "train_time": train_time
        })
        
        # Keep best model
        if oob_acc > best_accuracy:
            best_accuracy = oob_acc
            best_model = model
            best_config = config
    
    # Save results
    results_df = pd.DataFrame(results)
    results_df.to_csv(OUTPUT_DIR / 'ydf_tuning_results.csv', index=False)
    
    print(f"\nBest configuration: {best_config['name']} with OOB accuracy: {best_accuracy:.6f}")
    
    return best_model, best_config, results_df

def cross_validation(train_data):
    """5-fold cross-validation"""
    print("\n" + "="*60)
    print("5-FOLD CROSS-VALIDATION")
    print("="*60)
    
    # Prepare for sklearn CV
    X = train_data.drop('Personality', axis=1)
    y = train_data['Personality']
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Test different models
    models_to_test = [
        {"name": "RF-300", "learner": ydf.RandomForestLearner(label="Personality", num_trees=300)},
        {"name": "RF-500", "learner": ydf.RandomForestLearner(label="Personality", num_trees=500)},
        {"name": "GBT-300", "learner": ydf.GradientBoostedTreesLearner(label="Personality", num_trees=300)},
        {"name": "GBT-500-tuned", "learner": ydf.GradientBoostedTreesLearner(
            label="Personality", num_trees=500, shrinkage=0.05, max_depth=8
        )},
    ]
    
    cv_results = []
    
    for model_config in models_to_test:
        print(f"\nTesting {model_config['name']}...")
        fold_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
            # Split data
            train_fold = train_data.iloc[train_idx]
            val_fold = train_data.iloc[val_idx]
            
            # Train
            model = model_config['learner'].train(train_fold)
            
            # Evaluate
            val_accuracy = model.evaluate(val_fold).accuracy
            fold_scores.append(val_accuracy)
            
            print(f"  Fold {fold}: {val_accuracy:.6f}")
        
        # Calculate statistics
        mean_acc = np.mean(fold_scores)
        std_acc = np.std(fold_scores)
        
        print(f"  Mean: {mean_acc:.6f} (+/- {std_acc:.6f})")
        
        cv_results.append({
            "model": model_config['name'],
            "mean_accuracy": mean_acc,
            "std_accuracy": std_acc,
            "min_accuracy": min(fold_scores),
            "max_accuracy": max(fold_scores)
        })
    
    # Save results
    cv_df = pd.DataFrame(cv_results)
    cv_df.to_csv(OUTPUT_DIR / 'ydf_cv_results.csv', index=False)
    
    # Plot results
    plt.figure(figsize=(10, 6))
    models = cv_df['model']
    means = cv_df['mean_accuracy']
    stds = cv_df['std_accuracy']
    
    x = np.arange(len(models))
    plt.bar(x, means, yerr=stds, capsize=10, alpha=0.7)
    plt.xticks(x, models)
    plt.ylabel('Accuracy')
    plt.title('5-Fold Cross-Validation Results')
    plt.grid(True, axis='y', alpha=0.3)
    
    # Add value labels
    for i, (mean, std) in enumerate(zip(means, stds)):
        plt.text(i, mean + std + 0.001, f'{mean:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'ydf_cv_results.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return cv_df

def analyze_predictions(model, test_data, test_df):
    """Analyze model predictions"""
    print("\n" + "="*60)
    print("PREDICTION ANALYSIS")
    print("="*60)
    
    # Get predictions
    predictions_proba = model.predict(test_data)
    predictions = ['Extrovert' if p > 0.5 else 'Introvert' for p in predictions_proba]
    
    # Calculate uncertainty
    uncertainty = 1 - np.abs(predictions_proba - 0.5) * 2
    
    # Create analysis dataframe
    analysis = pd.DataFrame({
        'id': test_df['id'],
        'prediction': predictions,
        'extrovert_proba': predictions_proba,
        'uncertainty': uncertainty
    })
    
    # Find most uncertain
    print("\nMost uncertain predictions:")
    most_uncertain = analysis.nlargest(15, 'uncertainty')
    for _, row in most_uncertain.iterrows():
        print(f"  ID {int(row['id'])}: {row['extrovert_proba']:.3f} (uncertainty: {row['uncertainty']:.3f})")
    
    # Check if our known IDs are uncertain
    known_ids = [20934, 24005, 18754, 19482, 23711, 22291]
    print("\nChecking known IDs:")
    for known_id in known_ids:
        if known_id in analysis['id'].values:
            row = analysis[analysis['id'] == known_id].iloc[0]
            print(f"  ID {known_id}: {row['extrovert_proba']:.3f} (uncertainty: {row['uncertainty']:.3f})")
    
    # Save analysis
    analysis.to_csv(OUTPUT_DIR / 'ydf_final_analysis.csv', index=False)
    
    return analysis

def create_submissions(model, test_data, test_df, config_name):
    """Create submission files"""
    print("\n" + "="*60)
    print("CREATING SUBMISSIONS")
    print("="*60)
    
    # Get predictions
    predictions_proba = model.predict(test_data)
    predictions = ['Extrovert' if p > 0.5 else 'Introvert' for p in predictions_proba]
    
    # Standard submission
    submission = pd.DataFrame({
        'id': test_df['id'],
        'Personality': predictions
    })
    
    filename = f"ydf_optimized_{config_name.lower().replace(' ', '_')}.csv"
    submission.to_csv(WORKSPACE_DIR / "scores" / filename, index=False)
    print(f"Created: {filename}")
    
    # Also create a flip submission for most uncertain
    uncertainty = 1 - np.abs(predictions_proba - 0.5) * 2
    most_uncertain_idx = np.argmax(uncertainty)
    most_uncertain_id = test_df.iloc[most_uncertain_idx]['id']
    
    # Flip the most uncertain
    flip_submission = submission.copy()
    current_pred = flip_submission[flip_submission['id'] == most_uncertain_id]['Personality'].values[0]
    new_pred = 'Introvert' if current_pred == 'Extrovert' else 'Extrovert'
    flip_submission.loc[flip_submission['id'] == most_uncertain_id, 'Personality'] = new_pred
    
    flip_filename = f"ydf_flip_uncertain_id_{most_uncertain_id}.csv"
    flip_submission.to_csv(WORKSPACE_DIR / "scores" / flip_filename, index=False)
    print(f"Created: {flip_filename} (flipped {current_pred} â†’ {new_pred})")

def main():
    # Load data
    train_data, test_data, train_df, test_df = prepare_data()
    
    # 1. Hyperparameter tuning
    best_model, best_config, tuning_results = hyperparameter_tuning(train_data)
    
    # 2. Cross-validation
    cv_results = cross_validation(train_data)
    
    # 3. Analyze predictions
    analysis = analyze_predictions(best_model, test_data, test_df)
    
    # 4. Create submissions
    create_submissions(best_model, test_data, test_df, best_config['name'])
    
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print(f"\nBest model: {best_config['name']}")
    print(f"OOB Accuracy: {best_model.self_evaluation().accuracy:.6f}")
    print("\nCross-validation results:")
    print(cv_results.to_string(index=False))
    print("\nYDF advantages demonstrated:")
    print("- Fast training (all models < 2s)")
    print("- Built-in OOB evaluation")
    print("- No preprocessing needed")
    print("- Handles missing values natively")

if __name__ == "__main__":
    main()